{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pongpattt/DADS6005_PySpark_Quiz2/blob/main/DADS6005_Week12_Quiz2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxRUnxF4PBdy"
      },
      "outputs": [],
      "source": [
        "%%capture \n",
        "## don't show detail below by using \"%%capture\" \n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://mirrors.estointernet.in/apache/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar -xvf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install findspark  ## for python can use spark properly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget \"https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.1/spark-sql-kafka-0-10_2.12-3.3.1.jar\"\n",
        "!wget \"https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10_2.12/3.1.3/spark-streaming-kafka-0-10_2.12-3.1.3.jar\""
      ],
      "metadata": {
        "id": "CWsoQhb0NZ2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.3,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1 pyspark-shell'"
      ],
      "metadata": {
        "id": "m2ph4QQcQDTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *"
      ],
      "metadata": {
        "id": "9C4yFGwjQKvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz1"
      ],
      "metadata": {
        "id": "Ws8LOzR1sspL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Producer_V2.ipynb\n",
        "### Create some data through kafka"
      ],
      "metadata": {
        "id": "AaxEPBtIdJAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install confluent_kafka"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHeSdWbMdN4a",
        "outputId": "24b24fd7-51ed-4d95-d2b6-45637c968005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting confluent_kafka\n",
            "  Downloading confluent_kafka-1.9.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 32.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: confluent-kafka\n",
            "Successfully installed confluent-kafka-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Producer\n",
        "import time\n",
        "import json\n",
        "from uuid import uuid4\n",
        "\n",
        "p = Producer({'bootstrap.servers': 'ec2-13-229-46-113.ap-southeast-1.compute.amazonaws.com:9092'})\n",
        "\n",
        "def delivery_report(err, msg):\n",
        "    \"\"\" Called once for each message produced to indicate delivery result.\n",
        "        Triggered by poll() or flush(). \"\"\"\n",
        "    if err is not None:\n",
        "        print('Message delivery failed: {}'.format(err))\n",
        "    else:\n",
        "        print('Message delivered to {}'.format(msg.value().decode('utf-8')))\n",
        "\n",
        "#file1 = open('message.txt', 'r') \n",
        "#Lines = ['a a a','b b','c c c c c'] #file1.readlines()\n",
        "\n",
        "#jsonString1 = \"\"\"name:Gal,age:20\"\"\"\n",
        "#jsonString2 = \"\"\"name:Pok,age:25\"\"\"\n",
        "#jsonString3 = \"\"\"name:Vyy,age:30\"\"\"\n",
        "jsonString4 = \"\"\" {\"id\":2,\"firstname\":\"Aof\",\"middlename\":\"\",\"lastname\":\"Parker3\",\"dob_year\":2018,\"dob_month\":1,\"gender\":\"M\",\"salary\":3000} \"\"\"\n",
        "jsonv4 = jsonString4.encode()\n",
        "jsonString5 = \"\"\" {\"id\":1,\"firstname\":\"Cin\",\"middlename\":\"\",\"lastname\":\"Parker3\",\"dob_year\":2018,\"dob_month\":1,\"gender\":\"M\",\"salary\":4000} \"\"\"\n",
        "jsonv5 = jsonString5.encode()\n",
        "jsonString6 = \"\"\" {\"id\":3,\"firstname\":\"Boss\",\"middlename\":\"\",\"lastname\":\"Parker3\",\"dob_year\":2018,\"dob_month\":1,\"gender\":\"M\",\"salary\":5000} \"\"\"\n",
        "jsonv6 = jsonString6.encode()\n",
        "\n",
        "#jsonv1 = jsonString1.encode()\n",
        "#jsonv2 = jsonString2.encode()\n",
        "#jsonv3 = jsonString3.encode()\n",
        "#Lines = [jsonv1, jsonv2, jsonv3]\n",
        "\n",
        "Lines = [jsonv4, jsonv5, jsonv6]\n",
        "for line in Lines:\n",
        "    p.poll(0)\n",
        "    sendMsg = line #line.encode().decode('utf-8').strip('\\n')\n",
        "    p.produce('topic-dads6005-4-2', key=str(uuid4()), value=sendMsg , callback=delivery_report)  ## uuid : generate unique key\n",
        "    time.sleep(1)\n",
        "\n",
        "p.flush()"
      ],
      "metadata": {
        "id": "mER71F9PdRyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# another data\n",
        "\n",
        "from confluent_kafka import Producer\n",
        "import time\n",
        "import json\n",
        "from uuid import uuid4\n",
        "\n",
        "p = Producer({'bootstrap.servers': 'ec2-13-229-46-113.ap-southeast-1.compute.amazonaws.com:9092'})\n",
        "\n",
        "def delivery_report(err, msg):\n",
        "    \"\"\" Called once for each message produced to indicate delivery result.\n",
        "        Triggered by poll() or flush(). \"\"\"\n",
        "    if err is not None:\n",
        "        print('Message delivery failed: {}'.format(err))\n",
        "    else:\n",
        "        print('Message delivered to {}'.format(msg.value().decode('utf-8')))\n",
        "\n",
        "#file1 = open('message.txt', 'r') \n",
        "#Lines = ['a a a','b b','c c c c c'] #file1.readlines()\n",
        "\n",
        "#jsonString1 = \"\"\"name:Gal,age:20\"\"\"\n",
        "#jsonString2 = \"\"\"name:Pok,age:25\"\"\"\n",
        "#jsonString3 = \"\"\"name:Vyy,age:30\"\"\"\n",
        "jsonString7 = \"\"\" {\"id\":2,\"hobby\": \"Football\"} \"\"\"\n",
        "jsonv7 = jsonString7.encode()\n",
        "jsonString8 = \"\"\" {\"id\":1,\"hobby\": \"Tennis\"} \"\"\"\n",
        "jsonv8 = jsonString8.encode()\n",
        "jsonString9 = \"\"\" {\"id\":3,\"hobby\": \"Pingpong\"} \"\"\"\n",
        "jsonv9 = jsonString9.encode()\n",
        "\n",
        "#jsonv1 = jsonString1.encode()\n",
        "#jsonv2 = jsonString2.encode()\n",
        "#jsonv3 = jsonString3.encode()\n",
        "#Lines = [jsonv1, jsonv2, jsonv3]\n",
        "\n",
        "Lines = [jsonv7, jsonv8, jsonv9]\n",
        "for line in Lines:\n",
        "    p.poll(0)\n",
        "    sendMsg = line #line.encode().decode('utf-8').strip('\\n')\n",
        "    p.produce('topic-dads6005-4-4', key=str(uuid4()), value=sendMsg , callback=delivery_report)  ## uuid : generate unique key\n",
        "    time.sleep(1)\n",
        "\n",
        "p.flush()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33ttKgW5v7SH",
        "outputId": "cb5127eb-972a-4d38-ff9b-22136d9a37e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message delivered to  {\"id\":2,\"hobby\": \"Football\"} \n",
            "Message delivered to  {\"id\":1,\"hobby\": \"Tennis\"} \n",
            "Message delivered to  {\"id\":3,\"hobby\": \"Pingpong\"} \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pyspark_join_multiple_dstreams_json2dataframe.ipynb\n",
        "### start to Join 2 dataframes"
      ],
      "metadata": {
        "id": "FCbUh57hoJl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://mirrors.estointernet.in/apache/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!tar -xvf spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install findspark"
      ],
      "metadata": {
        "id": "3azmQCRFjkEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33dc5226-8c7e-4d3d-a845-d3f36534a197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-11-24 03:27:52\n",
            "-------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget \"https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.8/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar\""
      ],
      "metadata": {
        "id": "rMKoBDAkodi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.8-bin-hadoop2.7\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar pyspark-shell'"
      ],
      "metadata": {
        "id": "gkCMZCWyodlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import Normalizer, StandardScaler\n",
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "import json"
      ],
      "metadata": {
        "id": "Q5ykcu42odoU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25833c92-4ec9-438b-b27a-7bf3c67fd31e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2022-11-24 03:28:22\n",
            "-------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kafka_topic_name = \"topic-dads6005-4-1\"\n",
        "kafka_topic_name2 = \"topic-dads6005-4-2\"\n",
        "kafka_topic_name3 = \"topic-dads6005-4-3\"\n",
        "\n",
        "kafka_topic_name_ = \"topic-dads6005-4-4\"\n",
        "\n",
        "kafka_topic_name_cin = \"cin3\"\n",
        "\n",
        "kafka_topic_name_aof = \"aof\"\n",
        "\n",
        "kafka_bootstrap_servers = 'ec2-13-229-46-113.ap-southeast-1.compute.amazonaws.com:9092'"
      ],
      "metadata": {
        "id": "f2IzHtzOodqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex.1\n",
        "\n",
        "def readMyStream(rdd):\n",
        "  if not rdd.isEmpty():\n",
        "    df = spark.read.json(rdd)\n",
        "    print(df.dtypes)\n",
        "    print('Started the Process')\n",
        "    print('Selection of Columns')\n",
        "    df = df.select('firstname','lastname')\n",
        "    df.show()"
      ],
      "metadata": {
        "id": "ZdptI1KzonLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex.2\n",
        "\n",
        "def readMyStream2(rdd):\n",
        "  if not rdd.isEmpty():\n",
        "    df = spark.read.json(rdd)\n",
        "\n",
        "    # The inferred schema can be visualized using the printSchema() method\n",
        "    df.printSchema()\n",
        "\n",
        "    # Creates a temporary view using the DataFrame\n",
        "    df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "    # SQL statements can be run by using the sql methods provided by spark\n",
        "    resultDF = spark.sql(\"SELECT SUM(salary) FROM people\")\n",
        "    resultDF.show()"
      ],
      "metadata": {
        "id": "KfwsBxljonNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readMyStream_test(rdd, rdd2):\n",
        "  if not rdd.isEmpty() and not rdd2.isEmpty():\n",
        "    df = spark.read.json(rdd)\n",
        "    df2 = spark.read.json(rdd2)\n",
        "\n",
        "    # The inferred schema can be visualized using the printSchema() method\n",
        "    df.printSchema()\n",
        "    df2.printSchema()\n",
        "\n",
        "    # Creates a temporary view using the DataFrame\n",
        "    df.createOrReplaceTempView(\"kvs3_\")\n",
        "    df2.createOrReplaceTempView(\"kvs4_\")\n",
        "\n",
        "    # SQL statements can be run by using the sql methods provided by spark\n",
        "    #resultDF = spark.sql(\"SELECT kvs3_.id, kvs4_.firstname ,kvs3_travel FROM kvs3_ LEFT JOIN kvs4_ ON kvs3_.id = kvs4_.id \")\n",
        "    resultDF = spark.sql(\"SELECT * from kvs3_\")\n",
        "    resultDF.show()\n",
        "\n",
        "    resultDF2 = spark.sql(\"SELECT * from kvs4_\")\n",
        "    resultDF2.show()\n",
        "\n",
        "    resultDF3 = spark.sql(\"SELECT kvs3_.id, kvs4_.firstname, kvs3_.hobby from kvs3_ LEFT JOIN kvs4_ ON kvs3_.id = kvs4_.id ORDER BY kvs3_.id\")\n",
        "    resultDF3.show()"
      ],
      "metadata": {
        "id": "C1R7cVxs0giA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readMyStream_test2(rdd):\n",
        "  if not rdd.isEmpty():\n",
        "    df = spark.read.json(rdd)\n",
        "    # print(df.dtypes)\n",
        "    # print('Started the Process')\n",
        "    # print('Selection of Columns')\n",
        "    df = df.select('id', 'firstname', 'hobby')\n",
        "    df.show()"
      ],
      "metadata": {
        "id": "RRudcj0LOc7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = pyspark.SparkContext()\n",
        "spark = SparkSession(sc)\n",
        "ssc = StreamingContext(sc,4) # Show results every 2 seconds"
      ],
      "metadata": {
        "id": "78lcnzexonI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kvs1 = KafkaUtils.createDirectStream(ssc, \n",
        "                                    [kafka_topic_name], \n",
        "                                    {\n",
        "                        'bootstrap.servers':kafka_bootstrap_servers,\n",
        "                        'group.id':'test-group',\n",
        "                        'auto.offset.reset':'smallest' # largest\n",
        "                        })\n",
        "\n",
        "kvs2 = KafkaUtils.createDirectStream(ssc, \n",
        "                                    [kafka_topic_name2], \n",
        "                                    {\n",
        "                        'bootstrap.servers':kafka_bootstrap_servers,\n",
        "                        'group.id':'test-group',\n",
        "                        'auto.offset.reset':'smallest'\n",
        "                        })\n",
        "\n",
        "kvs3 = KafkaUtils.createDirectStream(ssc, \n",
        "                                    [kafka_topic_name_], # kafka_topic_name3\n",
        "                                    {\n",
        "                        'bootstrap.servers':kafka_bootstrap_servers,\n",
        "                        'group.id':'test-group',\n",
        "                        'auto.offset.reset':'smallest'\n",
        "                        })\n",
        "\n",
        "kvs4 = KafkaUtils.createDirectStream(ssc, \n",
        "                                    [kafka_topic_name_cin],   \n",
        "                                    {\n",
        "                        'bootstrap.servers':kafka_bootstrap_servers,\n",
        "                        'group.id':'test-group',\n",
        "                        'auto.offset.reset':'smallest'\n",
        "                        })\n",
        "\n",
        "#kvs2.pprint(100)\n",
        "#parsed = kvs2.map(lambda x: x[1])\n",
        "#parsed.foreachRDD( lambda rdd: readMyStream2(rdd) )\n",
        "\n",
        "kvs3.pprint(50)\n",
        "kvs4.pprint(50)\n",
        "\n",
        "kvs3_ = kvs3.map(lambda x: x[1])\n",
        "kvs4_ = kvs4.map(lambda x: x[1]).countByValue().map(lambda x: x[0])\n",
        "\n",
        "kvs3_.union(kvs4_).pprint(100)\n",
        "#kvs3_.union(kvs4_).foreachRDD( lambda rdd: rdd.items() )\n",
        "\n",
        "#var1 = kvs3_.union(kvs4_).glom()\n",
        "#var1.pprint(100)\n",
        "\n",
        "#kvs3_.join(kvs4_).pprint(100)\n",
        "\n",
        "kvs3_.transformWith(readMyStream_test, kvs4_).pprint(100)\n",
        "\n",
        "kvs3_rdd = kvs3_.map(lambda x: x.replace(\"{\", \"\").replace(\"}\", \"\").replace('\"', '').split('id:'))\n",
        "kvs3_rdd = kvs3_rdd.map(lambda x: x[1]+x[0]).map(lambda x: (x[0], x[2:].strip())).map(lambda x: (x[0],{ i.split(':')[0]:i.split(':')[1] for i in x[1].split(',') }) )\n",
        "\n",
        "kvs4_rdd = kvs4_.map(lambda x: x.replace(\"{\", \"\").replace(\"}\", \"\").replace('\"', '').split('id:'))\n",
        "kvs4_rdd = kvs4_rdd.map(lambda x: x[1]+x[0]).map(lambda x: (x[0], x[2:].strip())).map(lambda x: (x[0],{ i.split(':')[0]:i.split(':')[1] for i in x[1].split(',') }) )\n",
        "\n",
        "# kvs3_rdd.pprint(100)\n",
        "# kvs4_rdd.pprint(100)\n",
        "\n",
        "blank_dict = {}\n",
        "kvs_all = kvs3_rdd.join(kvs4_rdd).map(lambda x: ({\"id\":x[0]}, {**x[1][0], **x[1][1]})).map(lambda x: {**x[0], **x[1]})\n",
        "#.groupByKey().map(lambda x: (x[0], list(x[1]))).map(lambda x: x[0], ( {**i} for i in x[1])  ) #.map(lambda x: (x[0], ( {**i} for i in list(x[1]))  ))   #.cogroup(kvs4_rdd).map(lambda x : x[0], list(x[1]))\n",
        "kvs_all.pprint(100)\n",
        "\n",
        "kvs_all.foreachRDD(lambda rdd: readMyStream_test2(rdd))\n",
        "\n",
        "\n",
        "ssc.start()\n",
        "ssc.awaitTerminationOrTimeout(10)\n",
        "ssc.stop()\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOB9AWW8onP7",
        "outputId": "9ad5e318-b6e0-4ff3-bca5-cdf7111de0d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- hobby: string (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            "\n",
            "root\n",
            " |-- dob_month: long (nullable = true)\n",
            " |-- dob_year: long (nullable = true)\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+--------+---+\n",
            "|   hobby| id|\n",
            "+--------+---+\n",
            "|Football|  2|\n",
            "|  Tennis|  1|\n",
            "|Pingpong|  3|\n",
            "+--------+---+\n",
            "\n",
            "+---------+--------+---------+------+---+--------+----------+------+\n",
            "|dob_month|dob_year|firstname|gender| id|lastname|middlename|salary|\n",
            "+---------+--------+---------+------+---+--------+----------+------+\n",
            "|        1|    2018|      Cin|     M|  1| Parker3|          |  4000|\n",
            "|        1|    2018|     Boss|     M|  3| Parker3|          |  5000|\n",
            "|        1|    2018|      Aof|     M|  2| Parker3|          |  3000|\n",
            "+---------+--------+---------+------+---+--------+----------+------+\n",
            "\n",
            "+---+---------+--------+\n",
            "| id|firstname|   hobby|\n",
            "+---+---------+--------+\n",
            "|  1|      Cin|  Tennis|\n",
            "|  2|      Aof|Football|\n",
            "|  3|     Boss|Pingpong|\n",
            "+---+---------+--------+\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-11-08 12:58:24\n",
            "-------------------------------------------\n",
            "('a136cc8f-7840-4bed-9508-8139961ba2b3', ' {\"id\":2,\"hobby\": \"Football\"} ')\n",
            "('3ef1e74e-c41c-4b0d-9a0b-9a4af53cb378', ' {\"id\":1,\"hobby\": \"Tennis\"} ')\n",
            "('30f1d784-7544-4186-a03c-1b9727e3a3cf', ' {\"id\":3,\"hobby\": \"Pingpong\"} ')\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-11-08 12:58:24\n",
            "-------------------------------------------\n",
            "('84796261-da69-4039-9f26-c62aea1bfd9a', ' {\"id\":2,\"firstname\":\"Aof\",\"middlename\":\"\",\"lastname\":\"Parker3\",\"dob_year\":2018,\"dob_month\":1,\"gender\":\"M\",\"salary\":3000} ')\n",
            "('1a81c6ca-5f58-4586-be3e-deef3084a96d', ' {\"id\":1,\"firstname\":\"Cin\",\"middlename\":\"\",\"lastname\":\"Parker3\",\"dob_year\":2018,\"dob_month\":1,\"gender\":\"M\",\"salary\":4000} ')\n",
            "('09f2286c-9a8b-4554-8d3c-e3fdde26104a', ' {\"id\":3,\"firstname\":\"Boss\",\"middlename\":\"\",\"lastname\":\"Parker3\",\"dob_year\":2018,\"dob_month\":1,\"gender\":\"M\",\"salary\":5000} ')\n",
            "('6becdc7e-c8df-431e-a17f-3a591e24f87a', ' {\"id\":2,\"firstname\":\"Aof\",\"middlename\":\"\",\"lastname\":\"Parker3\",\"dob_year\":2018,\"dob_month\":1,\"gender\":\"M\",\"salary\":3000} ')\n",
            "('64687a11-5f7b-4dff-a8eb-2074e38885d5', ' {\"id\":1,\"firstname\":\"Cin\",\"middlename\":\"\",\"lastname\":\"Parker3\",\"dob_year\":2018,\"dob_month\":1,\"gender\":\"M\",\"salary\":4000} ')\n",
            "('ab12d80d-8189-4188-ab7f-b912e0447c2c', ' {\"id\":3,\"firstname\":\"Boss\",\"middlename\":\"\",\"lastname\":\"Parker3\",\"dob_year\":2018,\"dob_month\":1,\"gender\":\"M\",\"salary\":5000} ')\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-11-08 12:58:24\n",
            "-------------------------------------------\n",
            " {\"id\":2,\"hobby\": \"Football\"} \n",
            " {\"id\":1,\"hobby\": \"Tennis\"} \n",
            " {\"id\":3,\"hobby\": \"Pingpong\"} \n",
            " {\"id\":1,\"firstname\":\"Cin\",\"middlename\":\"\",\"lastname\":\"Parker3\",\"dob_year\":2018,\"dob_month\":1,\"gender\":\"M\",\"salary\":4000} \n",
            " {\"id\":3,\"firstname\":\"Boss\",\"middlename\":\"\",\"lastname\":\"Parker3\",\"dob_year\":2018,\"dob_month\":1,\"gender\":\"M\",\"salary\":5000} \n",
            " {\"id\":2,\"firstname\":\"Aof\",\"middlename\":\"\",\"lastname\":\"Parker3\",\"dob_year\":2018,\"dob_month\":1,\"gender\":\"M\",\"salary\":3000} \n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-11-08 12:58:24\n",
            "-------------------------------------------\n",
            "{'id': '1', 'hobby': ' Tennis', 'firstname': 'Cin', 'middlename': '', 'lastname': 'Parker3', 'dob_year': '2018', 'dob_month': '1', 'gender': 'M', 'salary': '4000'}\n",
            "{'id': '2', 'hobby': ' Football', 'firstname': 'Aof', 'middlename': '', 'lastname': 'Parker3', 'dob_year': '2018', 'dob_month': '1', 'gender': 'M', 'salary': '3000'}\n",
            "{'id': '3', 'hobby': ' Pingpong', 'firstname': 'Boss', 'middlename': '', 'lastname': 'Parker3', 'dob_year': '2018', 'dob_month': '1', 'gender': 'M', 'salary': '5000'}\n",
            "\n",
            "+---+---------+---------+\n",
            "| id|firstname|    hobby|\n",
            "+---+---------+---------+\n",
            "|  1|      Cin|   Tennis|\n",
            "|  2|      Aof| Football|\n",
            "|  3|     Boss| Pingpong|\n",
            "+---+---------+---------+\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-11-08 12:58:28\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-11-08 12:58:28\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-11-08 12:58:28\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-11-08 12:58:28\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-11-08 12:58:32\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-11-08 12:58:32\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-11-08 12:58:32\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2022-11-08 12:58:32\n",
            "-------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ssc.stop()\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "9D3O6bLZovPD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}